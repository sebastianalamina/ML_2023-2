{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Usando el archivo MNIST(mnist_train_small.csv)"
      ],
      "metadata": {
        "id": "T8hABvKYnBF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importamos las lib que usaremos\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import  Counter"
      ],
      "metadata": {
        "id": "6QMiguTriyzt"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (a) Realiza una reducción dimensional a dos componentes principales. De todo el conjunto de datos."
      ],
      "metadata": {
        "id": "G1sB3gkVm4aS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('./sample_data/mnist_train_small.csv')#Leemos el archivo especificado\n",
        "data = data.to_numpy() # Pasamos data de pandas a una matriz de numpy\n",
        "X = data[:,0:]  #seleccionamos todas las filas y columnas a partir de la segunda columna hasta el final.\n",
        "y = data[:,0]   #seleccionamos todos los elementos de la primer columna"
      ],
      "metadata": {
        "id": "eiSZMuLCwlD5"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "El argumento n_components=2 en la función PCA indica que se deben seleccionar solamente las dos componentes principales más importantes\n",
        "para la visualización de los datos. En otras palabras, la función PCA(n_components=2) transforma los datos originales en un conjunto de \n",
        "datos de dos dimensiones que mantiene la mayor cantidad de información posible. Esto se logra mediante la proyección de los datos originales\n",
        "en un espacio de dos dimensiones que maximiza la varianza de los datos.\n",
        "'''\n",
        "pca = PCA(n_components=2)"
      ],
      "metadata": {
        "id": "6UZ9yH_Di0sx"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "El resultado de pca.fit_transform(data) es una nueva matriz que representa los datos \n",
        "originales en un espacio de características con una dimensión reducida, en este caso a dos dimensiones.\n",
        "'''\n",
        "X_2d = pca.fit_transform(data)"
      ],
      "metadata": {
        "id": "77xb7K3ii17o"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenemos los datos correspondientes a la clase 0 y 1, así como las variables auxiliares para graficar los datos\n",
        "X0 = []\n",
        "X1 = []\n",
        "X0_x,X0_y,X1_x,X1_y = [],[],[],[]\n",
        "for c, x in zip(y,X_2d):\n",
        "  if c==0:\n",
        "    X0.append(x)\n",
        "    X0_x.append(x[0])\n",
        "    X0_y.append(x[1])\n",
        "  elif c==1:\n",
        "    X1.append(x)\n",
        "    X1_x.append(x[0])\n",
        "    X1_y.append(x[1])"
      ],
      "metadata": {
        "id": "qTbVDHhz7m1p"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generamos una grafica,tal que los puntos rojos pertenecen a la clase cero y los azules ala clase uno\n",
        "plt.scatter(X0_x, X0_y,c=\"r\")\n",
        "plt.scatter(X1_x, X1_y, c=\"b\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4Fize3rt-M8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Funcion auxiliar\n",
        "def getOnesZeros(n):\n",
        "  return np.zeros(n).tolist()+np.ones(n).tolist()"
      ],
      "metadata": {
        "id": "sxVIl2loeK2m"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (b) Separen los datos a cuyas etiqueta correspondan el 0 y 1. De tal forma que a cada etiqueta correspondan 50 muestras. Para crear un conjunto de entrenamiento.(100 Muestras)"
      ],
      "metadata": {
        "id": "bflXmFuHpkMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = X0[0:50]\n",
        "x1 = X1[0:50]\n",
        "X = x0+x1 \n",
        "y_train = np.array([int(y) for y in getOnesZeros(50)]) #clases correspondientes al conjunto de entrenamiento\n",
        "X_train = np.array([x.tolist() for x in X]) #conjunto de entrenamiento"
      ],
      "metadata": {
        "id": "sXPi_165CmXC"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (c) Gráfica la malla de la clasificación y los puntos de los datos completos de ambas etiquetas. Usando K-NN."
      ],
      "metadata": {
        "id": "-cEdC3pzpzqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos el modelo KNN y entrenamos\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Creamos una malla de puntos para clasificar\n",
        "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
        "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max), np.arange(y_min, y_max))\n",
        "\n",
        "# Predecimos la clase para cada punto en la malla\n",
        "c = np.c_[xx.ravel(), yy.ravel()]\n",
        "Z = knn.predict(c)\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Graficamos la malla de clasificación\n",
        "plt.figure(figsize=(8, 6))\n",
        "#plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
        "plt.pcolormesh(xx, yy, Z, alpha=0.4)\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k')\n",
        "plt.title('Malla de Clasificación para KNN')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2NAarjieQ6P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para calcular la distancia euclidiana\n",
        "def euclidean_distance(point, centroid):\n",
        "    return np.sqrt(np.sum((point - centroid)**2))\n",
        "#generamos el conjunto de prueba\n",
        "x0_test = X0[50:len(X0)]\n",
        "x1_test = X1[50:len(X1)]\n",
        "X_test = x0_test+x1_test\n",
        "print(len(X))\n",
        "X_test = np.array(X_test) # No se puede usar la funcion len() con arrays\n",
        "\n",
        "def k_nearest_neighbors(X_train, y_train, X_test, k=3):\n",
        "  #Lista para almacenar las clases asignadas\n",
        "  y_pred = []\n",
        "  X_test = np.array(X_test)\n",
        "\n",
        "  #Iterar sobre cada punto de prueba\n",
        "  for ind in range(X_test.shape[0]):\n",
        "    #Calcular las distancias entre el punto de prueba y todos los puntos de entrenamiento.\n",
        "    distancias = [euclidean_distance(X_test[ind], p_ent ) for p_ent in X_train ]\n",
        "    \n",
        "    #Seleccionar los K puntos más cercanos\n",
        "    K_indices = np.argsort(distancias)[:k] \n",
        "    \n",
        "    # Asignar la clase más frecuente entre los K vecinos más cercanos al punto de prueba\n",
        "    k_nearest_classes = [y_train[idx] for idx in K_indices]\n",
        "    \n",
        "    #Seleccionamos la clase más comun. [(valor,numero_de_repeticiones)]\n",
        "    most_common_clas = Counter(k_nearest_classes).most_common(1)\n",
        "    #Asigamos a la clase más commun.\n",
        "    y_pred.append(most_common_clas[0][0])\n",
        "  return y_pred\n",
        "\n",
        "k_nearest_neighbors(X_train, y_train, X_test,3)"
      ],
      "metadata": {
        "id": "8--pn_gksFeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = k_nearest_neighbors(X_train, y_train, X_test, k=3)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "Oj5hIsGTulOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,8),dpi=100)\n",
        "n_vec = 5\n",
        "#Predecir clases para cada punto en X_space\n",
        "y_space_pred = k_nearest_neighbors(X_train, y_train, X_test, k=n_vec)\n",
        "\n",
        "# Graficar los puntos de X_space en diferentes colores según su clase asignada\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_space_pred, cmap='Accent', alpha=.3)#label='Predicted Classes'\n",
        "\n",
        "#Se predice el conjunto de prueba\n",
        "y_test = k_nearest_neighbors(X_train, y_train, X_test, k=n_vec)\n",
        "\n",
        "# Graficar los puntos de entrenamiento y los puntos de prueba en diferentes colores\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='Accent', label='Training Points')\n",
        "#Conjunto de prueba\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1],marker='x', c=y_test,cmap='Accent', label='Test Points')\n",
        "plt.title(f'Vecinos k{n_vec}')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KtgUuTZzunhn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}