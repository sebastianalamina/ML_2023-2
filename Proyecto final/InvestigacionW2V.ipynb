{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj5OXfStHRAq"
      },
      "source": [
        "### Librerias Usadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wbLhTObWvAwO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "9ee27694-6ba5-466e-dd1d-e6d843a30456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import TruncatedSVD, PCA\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "import re, string, nltk\n",
        "# Descarga de recursos\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkxUETQbS7xx",
        "outputId": "0db676e5-94c8-4d01-bcb1-aac3959a2969"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiZdPaiEOU5P"
      },
      "source": [
        "## Lectura de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_DcuAIZKwf8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64ed9166-51b2-4615-af8c-aee698d82841"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fake:  (23481, 5)\n",
            "True:  (21417, 5)\n",
            "All data:  (44898, 2)\n"
          ]
        }
      ],
      "source": [
        "dataFake = pd.read_csv(\"/content/drive/MyDrive/Fake.csv\")\n",
        "dataFake[\"class\"] = 0\n",
        "print(\"Fake: \",dataFake.shape)\n",
        "\n",
        "dataTrue = pd.read_csv(\"/content/drive/MyDrive/True.csv\")\n",
        "dataTrue[\"class\"] = 1\n",
        "print(\"True: \",dataTrue.shape)\n",
        "\n",
        "data_merge = pd.concat([dataFake,dataTrue], axis=0)\n",
        "data = data_merge.drop([\"title\",\"subject\",\"date\"], axis=1)\n",
        "print(\"All data: \",data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2PeBL87OU5U"
      },
      "source": [
        "## Limpieza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JLoc_nj33pBL"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  Función que elimina los símbolos especiales de un texto,\n",
        "  así como las stopwords\n",
        "'''\n",
        "def word_cleaner(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub('\\[.*?\\]', '', text)\n",
        "  text = re.sub('\\\\W', ' ', text)\n",
        "  text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "  text = re.sub('<.*?>+', '', text)\n",
        "  text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "  text = re.sub('\\n', '', text)\n",
        "  text = re.sub('\\w*\\d\\w*', '', text)\n",
        "  return remove_stopwords(text)\n",
        "\n",
        "'''\n",
        "  Función que dado un texto, lo limpia y elimina las letras aisladas existentes.\n",
        "'''\n",
        "def text_cleaner(text, lemmatizer):\n",
        "  text = word_cleaner(text)\n",
        "  tokens = word_tokenize(text)\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "  text = remove_letters(lemmatized_tokens)\n",
        "  text = \" \".join(text)\n",
        "  return text\n",
        "\n",
        "'''\n",
        "  Funcion que elimina las palabras con lengitud menor a length\n",
        "'''\n",
        "def remove_letters(lemas, length=2):\n",
        "  return [word for word in lemas if len(word)>length]\n",
        "\n",
        "'''\n",
        "  Funcion que cuenta la frecuencia de palabras en el dataset\n",
        "'''\n",
        "def count_tokens(texts, wf):\n",
        "  for text in texts:\n",
        "    tokens = text.split()\n",
        "    wf.update(tokens)\n",
        "  return wf\n",
        "\n",
        "'''\n",
        "  Funcion que dado el dataset y la lista de palabras que no tienen una \n",
        "  frecuencua valida las elimina del dataset\n",
        "'''\n",
        "def remove_max_min_words_freq(texts, words_to_remove):\n",
        "  filtered_texts = []\n",
        "  for text in texts:\n",
        "    tokens = text.split()\n",
        "    filtered_tokens = [word for word in tokens if word not in words_to_remove]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    filtered_texts.append(filtered_text)\n",
        "  return filtered_texts\n",
        "\n",
        "'''\n",
        "  Funcion que obtiene la lista de palabras a eliminar\n",
        "'''\n",
        "def get_words_to_remove(min_freq =2, max_freq=1000, word_freq=None):\n",
        "  return [word for word, freq in word_freq.items() if freq < min_freq or freq > max_freq]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj6vfsFNIZWy"
      },
      "source": [
        "### Aplicación de limpieza a datos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "X0jFN8eZOU5V"
      },
      "outputs": [],
      "source": [
        "clean_data = data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Gw0EvHwy4lzZ"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "clean_data[\"text\"] = clean_data['text'].apply(text_cleaner, args=(lemmatizer,))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_fake = dataFake.copy()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "clean_fake['text'] = clean_fake['text'].apply(text_cleaner, args=(lemmatizer,))"
      ],
      "metadata": {
        "id": "wx-8gnumuMCq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_true = dataTrue.copy()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "clean_true['text'] = clean_true['text'].apply(text_cleaner, args=(lemmatizer,))"
      ],
      "metadata": {
        "id": "Y9UjrUA2w7XX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJLFNJikOU5X"
      },
      "source": [
        "## Separación de datos de entrenamiento y de prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWemUeuqOU5Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc88b20b-a5c6-4d8e-c78d-046ead8a1c10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos de entrenamiento: 33673 (74.998886%)\n",
            "Datos de prueba: \t11225 (25.001114%)\n"
          ]
        }
      ],
      "source": [
        "SEED = 123456789\n",
        "\n",
        "x = clean_data['text']\n",
        "y = clean_data['class']\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=SEED)\n",
        "\n",
        "print(f\"Datos de entrenamiento: {len(x_train)} ({len(x_train)/len(x):%})\")\n",
        "print(f\"Datos de prueba: \\t{len(x_test)} ({len(x_test)/len(x):%})\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 123456789\n",
        "\n",
        "x_true = clean_true['text']\n",
        "y_true = clean_true['class']\n",
        "\n",
        "x_train_true, x_test_true, y_train_true, y_test_true = train_test_split(x_true, y_true, test_size=0.25, random_state=SEED)\n",
        "\n",
        "print(f\"Datos de entrenamiento: {len(x_train_true)} ({len(x_train_true)/len(x_true):%})\")\n",
        "print(f\"Datos de prueba: \\t{len(x_test_true)} ({len(x_test_true)/len(x_true):%})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BZHiivx6z0C",
        "outputId": "84b11e86-563c-4c81-9f8d-2063db9fcdf4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos de entrenamiento: 16062 (74.996498%)\n",
            "Datos de prueba: \t5355 (25.003502%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_fake = clean_fake['text']\n",
        "y_fake = clean_fake['class']\n",
        "\n",
        "x_train_fake, x_test_fake, y_train_fake, y_test_fake = train_test_split(x_fake, y_fake, test_size=0.25, random_state=SEED)\n",
        "\n",
        "print(f\"Datos de entrenamiento: {len(x_train_fake)} ({len(x_train_fake)/len(x_fake):%})\")\n",
        "print(f\"Datos de prueba: \\t{len(x_test_fake)} ({len(x_test_fake)/len(x_fake):%})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22xMQgKU9f6x",
        "outputId": "d6de8b9f-f70b-4ca5-f599-604a638df553"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos de entrenamiento: 17610 (74.996806%)\n",
            "Datos de prueba: \t5871 (25.003194%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YwspivV7Tln"
      },
      "source": [
        "### Vectorización por Word2Vec\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.concat([x_train_true, x_test_true], axis=0)\n",
        "sentences = [text.split() for text in X]\n",
        "word2vec_model = Word2Vec(sentences, min_count=2)"
      ],
      "metadata": {
        "id": "3bpKM5FA7j21"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model.wv.index_to_key[:100]"
      ],
      "metadata": {
        "id": "AOoakMrP8V-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model.wv.most_similar('trump')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNx6ajLZ8h1U",
        "outputId": "1c465d51-5656-4e7c-b43c-48f6f27ea1cb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('elect', 0.6293635964393616),\n",
              " ('surprise', 0.489088237285614),\n",
              " ('republican', 0.4756985902786255),\n",
              " ('obama', 0.4649403989315033),\n",
              " ('washington', 0.45603862404823303),\n",
              " ('ayer', 0.45538219809532166),\n",
              " ('romney', 0.45269158482551575),\n",
              " ('clinton', 0.44878089427948),\n",
              " ('rubio', 0.43388909101486206),\n",
              " ('bush', 0.4323711097240448)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_fake = pd.concat([x_train_fake, x_test_fake], axis=0)\n",
        "sentences_fake = [text.split() for text in X_fake]\n",
        "word2vec_model_f = Word2Vec(sentences_fake, min_count=2)"
      ],
      "metadata": {
        "id": "G8bq4Bpo9cnw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model_f.wv.most_similar('trump')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqjDDDts-xpm",
        "outputId": "176b4e87-22ce-45e9-a7cf-84de79615d58"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('elect', 0.634766161441803),\n",
              " ('hasn', 0.5471111536026001),\n",
              " ('actually', 0.5372609496116638),\n",
              " ('proving', 0.5154432058334351),\n",
              " ('conway', 0.501865029335022),\n",
              " ('pathetic', 0.49315014481544495),\n",
              " ('loyal', 0.481100469827652),\n",
              " ('yammer', 0.4697333872318268),\n",
              " ('insisting', 0.4696291387081146),\n",
              " ('embarrassing', 0.46709489822387695)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_true = set(word2vec_model.wv.index_to_key)\n",
        "X_train_vect_true = np.array([np.array([word2vec_model.wv[i] for i in ls if i in words_true]) for ls in x_train_true])\n",
        "X_test_vect_true = np.array([np.array([word2vec_model.wv[i] for i in ls if i in words_true]) for ls in x_test_true])"
      ],
      "metadata": {
        "id": "xokG4wS0_KcD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vect_avg_true = []\n",
        "for v in X_train_vect_true:\n",
        "    if v.size:\n",
        "        X_train_vect_avg_true.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_train_vect_avg_true.append(np.zeros(1, dtype=float))\n",
        "        \n",
        "X_test_vect_avg_true = []\n",
        "for v in X_test_vect_true:\n",
        "    if v.size:\n",
        "        X_test_vect_avg_true.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_test_vect_avg_true.append(np.zeros(1, dtype=float))"
      ],
      "metadata": {
        "id": "cHqW_n9T_i71"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "rf_model = rf.fit(X_train_vect_avg_true, y_train_true.values.ravel())"
      ],
      "metadata": {
        "id": "21NR8IXRAYV-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = rf_model.predict(X_test_vect_avg_true)"
      ],
      "metadata": {
        "id": "ATE_NfzKCA-x"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "precision = precision_score(y_test_true, y_pred)\n",
        "recall = recall_score(y_test_true, y_pred)\n",
        "print('Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(precision, 3), round(recall, 3), round((y_pred==y_test_true).sum()/len(y_pred), 3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb5Rhc5ZCFE9",
        "outputId": "dd9065f0-c2db-4fad-fd11-72ed5481cd39"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0 / Recall: 1.0 / Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_fake = set(word2vec_model_f.wv.index_to_key)\n",
        "X_train_vect_fake = np.array([np.array([word2vec_model_f.wv[i] for i in ls if i in words_fake]) for ls in x_train_fake])\n",
        "X_test_vect_fake = np.array([np.array([word2vec_model_f.wv[i] for i in ls if i in words_fake]) for ls in x_test_fake])\n",
        "\n",
        "X_train_vect_avg_fake = []\n",
        "for v in X_train_vect_fake:\n",
        "    if v.size:\n",
        "        X_train_vect_avg_fake.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_train_vect_avg_fake.append(np.zeros(10, dtype=float))\n",
        "        \n",
        "X_test_vect_avg_fake = []\n",
        "for v in X_test_vect_fake:\n",
        "    if v.size:\n",
        "        X_test_vect_avg_fake.append(v.mean(axis=0))\n",
        "    else:\n",
        "        X_test_vect_avg_fake.append(np.zeros(10, dtype=float))"
      ],
      "metadata": {
        "id": "fUfqHwVOC0H2"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_f = RandomForestClassifier()\n",
        "rf_model_f = rf_f.fit(X_train_vect_avg_fake, y_train_fake.values.ravel())\n",
        "\n",
        "y_pred_f = rf_model_f.predict(X_test_vect_avg_fake)"
      ],
      "metadata": {
        "id": "hV6yIPhrC4cq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision_f = precision_score(y_test_fake, y_pred_f)\n",
        "recall_f = recall_score(y_test_fake, y_pred_f)\n",
        "print('Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(precision_f, 3), round(recall_f, 3), round((y_pred_f == y_test_fake).sum()/len(y_pred_f), 3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "616jI0ilDpiy",
        "outputId": "fc4d7d84-723a-45ce-8440-190a58373771"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.0 / Recall: 0.0 / Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}