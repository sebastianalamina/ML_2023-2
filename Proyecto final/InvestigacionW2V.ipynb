{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj5OXfStHRAq"
      },
      "source": [
        "### Librerias Usadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "wbLhTObWvAwO",
        "outputId": "89fab7e9-49ba-4e8a-c8cb-1ec9941f7772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import TruncatedSVD, PCA\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "import re, string, nltk\n",
        "# Descarga de recursos\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkxUETQbS7xx",
        "outputId": "802bf158-e4d9-4fe4-e38e-929b2ddab2c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiZdPaiEOU5P"
      },
      "source": [
        "## Lectura de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DcuAIZKwf8K",
        "outputId": "6c59aeb0-3898-49e2-cee4-a13c1b324e28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fake:  (23481, 5)\n",
            "True:  (21417, 5)\n",
            "All data:  (44898, 2)\n"
          ]
        }
      ],
      "source": [
        "dataFake = pd.read_csv(\"/content/drive/MyDrive/Fake.csv\")\n",
        "dataFake[\"class\"] = 0\n",
        "print(\"Fake: \",dataFake.shape)\n",
        "\n",
        "dataTrue = pd.read_csv(\"/content/drive/MyDrive/True.csv\")\n",
        "dataTrue[\"class\"] = 1\n",
        "print(\"True: \",dataTrue.shape)\n",
        "\n",
        "data_merge = pd.concat([dataFake,dataTrue], axis=0)\n",
        "data = data_merge.drop([\"title\",\"subject\",\"date\"], axis=1)\n",
        "print(\"All data: \",data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2PeBL87OU5U"
      },
      "source": [
        "## Limpieza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JLoc_nj33pBL"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  Función que elimina los símbolos especiales de un texto,\n",
        "  así como las stopwords\n",
        "'''\n",
        "def word_cleaner(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub('\\[.*?\\]', '', text)\n",
        "  text = re.sub('\\\\W', ' ', text)\n",
        "  text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "  text = re.sub('<.*?>+', '', text)\n",
        "  text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "  text = re.sub('\\n', '', text)\n",
        "  text = re.sub('\\w*\\d\\w*', '', text)\n",
        "  return remove_stopwords(text)\n",
        "\n",
        "'''\n",
        "  Función que dado un texto, lo limpia y elimina las letras aisladas existentes.\n",
        "'''\n",
        "def text_cleaner(text, lemmatizer):\n",
        "  text = word_cleaner(text)\n",
        "  tokens = word_tokenize(text)\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "  text = remove_letters(lemmatized_tokens)\n",
        "  text = \" \".join(text)\n",
        "  return text\n",
        "\n",
        "'''\n",
        "  Funcion que elimina las palabras con lengitud menor a length\n",
        "'''\n",
        "def remove_letters(lemas, length=2):\n",
        "  return [word for word in lemas if len(word)>length]\n",
        "\n",
        "'''\n",
        "  Funcion que cuenta la frecuencia de palabras en el dataset\n",
        "'''\n",
        "def count_tokens(texts, wf):\n",
        "  for text in texts:\n",
        "    tokens = text.split()\n",
        "    wf.update(tokens)\n",
        "  return wf\n",
        "\n",
        "'''\n",
        "  Funcion que dado el dataset y la lista de palabras que no tienen una \n",
        "  frecuencua valida las elimina del dataset\n",
        "'''\n",
        "def remove_max_min_words_freq(texts, words_to_remove):\n",
        "  filtered_texts = []\n",
        "  for text in texts:\n",
        "    tokens = text.split()\n",
        "    filtered_tokens = [word for word in tokens if word not in words_to_remove]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    filtered_texts.append(filtered_text)\n",
        "  return filtered_texts\n",
        "\n",
        "'''\n",
        "  Funcion que obtiene la lista de palabras a eliminar\n",
        "'''\n",
        "def get_words_to_remove(min_freq =2, max_freq=1000, word_freq=None):\n",
        "  return [word for word, freq in word_freq.items() if freq < min_freq or freq > max_freq]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj6vfsFNIZWy"
      },
      "source": [
        "### Aplicación de limpieza a datos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0jFN8eZOU5V"
      },
      "outputs": [],
      "source": [
        "clean_data = data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gw0EvHwy4lzZ"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "clean_data[\"text\"] = clean_data['text'].apply(text_cleaner, args=(lemmatizer,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wx-8gnumuMCq"
      },
      "outputs": [],
      "source": [
        "clean_fake = dataFake.copy()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "clean_fake['text'] = clean_fake['text'].apply(text_cleaner, args=(lemmatizer,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y9UjrUA2w7XX"
      },
      "outputs": [],
      "source": [
        "clean_true = dataTrue.copy()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "clean_true['text'] = clean_true['text'].apply(text_cleaner, args=(lemmatizer,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJLFNJikOU5X"
      },
      "source": [
        "## Separación de datos de entrenamiento y de prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWemUeuqOU5Y"
      },
      "outputs": [],
      "source": [
        "SEED = 123456789\n",
        "\n",
        "x = clean_data['text']\n",
        "y = clean_data['class']\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=SEED)\n",
        "\n",
        "print(f\"Datos de entrenamiento: {len(x_train)} ({len(x_train)/len(x):%})\")\n",
        "print(f\"Datos de prueba: \\t{len(x_test)} ({len(x_test)/len(x):%})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BZHiivx6z0C",
        "outputId": "9b923664-ea06-4b1f-e2cd-a5ffca8de7e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos de entrenamiento: 16062 (74.996498%)\n",
            "Datos de prueba: \t5355 (25.003502%)\n"
          ]
        }
      ],
      "source": [
        "SEED = 123456789\n",
        "\n",
        "x_true = clean_true['text']\n",
        "y_true = clean_true['class']\n",
        "\n",
        "x_train_true, x_test_true, y_train_true, y_test_true = train_test_split(x_true, y_true, test_size=0.25, random_state=SEED)\n",
        "\n",
        "print(f\"Datos de entrenamiento: {len(x_train_true)} ({len(x_train_true)/len(x_true):%})\")\n",
        "print(f\"Datos de prueba: \\t{len(x_test_true)} ({len(x_test_true)/len(x_true):%})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22xMQgKU9f6x",
        "outputId": "9f983ed5-2050-484f-b686-9dd3981cea09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos de entrenamiento: 17610 (74.996806%)\n",
            "Datos de prueba: \t5871 (25.003194%)\n"
          ]
        }
      ],
      "source": [
        "x_fake = clean_fake['text']\n",
        "y_fake = clean_fake['class']\n",
        "\n",
        "x_train_fake, x_test_fake, y_train_fake, y_test_fake = train_test_split(x_fake, y_fake, test_size=0.25, random_state=SEED)\n",
        "\n",
        "print(f\"Datos de entrenamiento: {len(x_train_fake)} ({len(x_train_fake)/len(x_fake):%})\")\n",
        "print(f\"Datos de prueba: \\t{len(x_test_fake)} ({len(x_test_fake)/len(x_fake):%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YwspivV7Tln"
      },
      "source": [
        "### Word2Vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3bpKM5FA7j21"
      },
      "outputs": [],
      "source": [
        "X = pd.concat([x_train_true, x_test_true], axis=0)\n",
        "sentences = [text.split() for text in X]\n",
        "word2vec_model = Word2Vec(sentences, min_count=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOoakMrP8V-L"
      },
      "outputs": [],
      "source": [
        "word2vec_model.wv.index_to_key[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "G8bq4Bpo9cnw"
      },
      "outputs": [],
      "source": [
        "X_fake = pd.concat([x_train_fake, x_test_fake], axis=0)\n",
        "sentences_fake = [text.split() for text in X_fake]\n",
        "word2vec_model_f = Word2Vec(sentences_fake, min_count=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando la función `most_similar`, buscamos para cada modelo las palabras relacionadas a una de las que más repeticiones tuvieron (tanto en las noticias falsas como las verdaderas)\n",
        "\n"
      ],
      "metadata": {
        "id": "flFsySEoFd4J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNx6ajLZ8h1U",
        "outputId": "0d04c858-b9e7-477f-981f-a7df755877fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('elect', 0.6491174697875977),\n",
              " ('washington', 0.485962450504303),\n",
              " ('obama', 0.48377183079719543),\n",
              " ('bush', 0.4748460054397583),\n",
              " ('republican', 0.4728362262248993),\n",
              " ('surprise', 0.4660532474517822),\n",
              " ('clinton', 0.46550247073173523),\n",
              " ('incoming', 0.45599403977394104),\n",
              " ('presumptive', 0.45119884610176086),\n",
              " ('cruz', 0.4486404061317444)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "word2vec_model.wv.most_similar('trump')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqjDDDts-xpm",
        "outputId": "1ee666c4-7a18-49f9-9b6c-84475b1421fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('elect', 0.6708611845970154),\n",
              " ('actually', 0.5508329272270203),\n",
              " ('hasn', 0.5336906909942627),\n",
              " ('amateur', 0.5105623006820679),\n",
              " ('conway', 0.501033365726471),\n",
              " ('repeatedly', 0.49410584568977356),\n",
              " ('pathetic', 0.4832920730113983),\n",
              " ('embarrassing', 0.47700735926628113),\n",
              " ('brag', 0.4723009467124939),\n",
              " ('instead', 0.47010910511016846)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "word2vec_model_f.wv.most_similar('trump')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBdBd1hR2K7f",
        "outputId": "009c830c-5040-4bc3-d52c-25face089d25"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('elect',\n",
              " 'actually',\n",
              " 'hasn',\n",
              " 'amateur',\n",
              " 'conway',\n",
              " 'repeatedly',\n",
              " 'pathetic',\n",
              " 'embarrassing',\n",
              " 'brag',\n",
              " 'instead')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "list(zip(*word2vec_model_f.wv.most_similar('trump')))[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, apoyados de los resultados obtenidos en la investigación de `CountVectorizer`, se obtendrán las palabras relacionadas para cada una de las que más aparecieron en cada tipo de noticias."
      ],
      "metadata": {
        "id": "xE114qVjG3zq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "5UVKa6Y6yHmj"
      },
      "outputs": [],
      "source": [
        "fake_keywords = [\n",
        "    'medium', 'donald', 'black', 'video',\n",
        "    'woman', 'com', 'featured', 'news', \n",
        "    'america', 'twitter', 'obama', 'time',\n",
        "    'know', 'clinton', 'american', 'people',\n",
        "    'hillary', 'like', 'image', 'trump'\n",
        "]\n",
        "\n",
        "true_keywords = [\n",
        "    'said', 'reuters', 'state', 'government',\n",
        "    'minister', 'official', 'united', 'china',\n",
        "    'north', 'washington', 'party', 'republican',\n",
        "    'leader', 'korea', 'tax', 'wednesday', \n",
        "    'house', 'tuesday', 'percent', 'senate',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6mG5kFw0R_o",
        "outputId": "1195d6fb-be16-48b3-ffac-165a38bffaff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "medium ('trollinga', 'occasioned', 'waht', 'presentable', 'networking', 'macroeconomics', 'irredeemable', 'problemthough', 'gravitating', 'contagians')\n",
            "donald ('humiliating', 'candidacy', 'elect', 'surrogate', 'terrifying', 'pathetic', 'trashing', 'bluffing', 'supporter', 'proving')\n",
            "black ('color', 'assanta', 'cop', 'hispanic', 'angry', 'african', 'lynching', 'young', 'oppressed', 'racial')\n",
            "video ('clip', 'mediatate', 'footage', 'hamish', 'recording', 'prankster', 'audio', 'teamyoutube', 'reopens', 'hedgethis')\n",
            "woman ('men', 'sexually', 'female', 'kissing', 'rape', 'girl', 'nationalismthe', 'harassed', 'empowe', 'angriest')\n",
            "com ('http', 'braddjaffy', 'jaffy', 'huffpostpol', 'caplan', 'jacobnbc', 'jenniferjjacobs', 'joshdcaplan', 'clayaiken', 'yashar')\n",
            "featured ('hopefully', 'twittertwitterfeatured', 'dailypoliticsfeatured', 'rest', 'twitterwe', 'twitterfeatured', 'bet', 'anymore', 'scrolled', 'definitely')\n",
            "news ('newswatch', 'newshere', 'mccordsville', 'zedillo', 'newsbelow', 'newssorry', 'newswe', 'newswouldn', 'auenyoeiio', 'difabio')\n",
            "america ('nation', 'country', 'american', 'strength', 'world', 'legacy', 'britain', 'truly', 'unity', 'greatest')\n",
            "twitter ('unfollowed', 'fakepie', 'thefriddle', 'zit', 'nbcnewyork', 'yournewswire', 'carlyfiorinaforvicepresident', 'gmail', 'joevargas', 'gulfsails')\n",
            "obama ('obamaobama', 'huessein', 'obamae', 'zetouni', 'incoming', 'smore', 'molotovd', 'obamayou', 'bush', 'asterisk')\n",
            "time ('minute', 'hour', 'period', 'hasn', 'haven', 'short', 'term', 'actually', 'soon', 'overdue')\n",
            "know ('think', 'exactly', 'everybody', 'anybody', 'wouldn', 'maybe', 'understand', 'guess', 'frankly', 'knowing')\n",
            "clinton ('crooked', 'clintonhillary', 'caintv', 'sizzle', 'dmlol', 'demoratic', 'percolated', 'rodham', 'capitalizes', 'atergate')\n",
            "american ('america', 'francophone', 'phantasmal', 'ordinary', 'comedien', 'repute', 'belching', 'country', 'irenic', 'one')\n",
            "people ('person', 'folk', 'aren', 'anybody', 'one', 'afraid', 'feel', 'individual', 'frankly', 'realize')\n",
            "hillary ('wirewhat', 'rapistthe', 'grifter', 'chargeso', 'armanihillary', 'desensitized', 'gratis', 'foundationira', 'hawkishness', 'mcdougal')\n",
            "like ('pedagogically', 'bad', 'clintonesque', 'scary', 'way', 'doke', 'bite', 'comfortable', 'crazy', 'yeah')\n",
            "image ('imag', 'iamge', 'imaged', 'vivian', 'vibrator', 'dinapolithe', 'nomi', 'photo', 'flickr', 'merendino')\n",
            "trump ('elect', 'actually', 'hasn', 'amateur', 'conway', 'repeatedly', 'pathetic', 'embarrassing', 'brag', 'instead')\n"
          ]
        }
      ],
      "source": [
        "fake = dict()\n",
        "for word in fake_keywords:\n",
        "  #r.append(list(zip(*word2vec_model_f.wv.most_similar(word)))[0])\n",
        "  fake[word] =  list(zip(*word2vec_model_f.wv.most_similar(word)))[0]\n",
        "  print(word, list(zip(*word2vec_model_f.wv.most_similar(word)))[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true = dict()\n",
        "for word in true_keywords:\n",
        "  #r.append(list(zip(*word2vec_model_f.wv.most_similar(word)))[0])\n",
        "  true[word] =  list(zip(*word2vec_model.wv.most_similar(word)))[0]\n",
        "  print(word, list(zip(*word2vec_model.wv.most_similar(word)))[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTC9YZODE5M0",
        "outputId": "f75caf48-6a98-4625-d2b1-609fb73fccb2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "said ('added', 'told', 'adding', 'saying', 'noted', 'wrote', 'suggested', 'asked', 'described', 'acknowledged')\n",
            "reuters ('morseby', 'bacalar', 'reporter', 'creighton', 'bfm', 'inquirer', 'based', 'transcribed', 'telephone', 'odell')\n",
            "state ('nation', 'kingdom', 'exportation', 'emirate', 'steelworker', 'continental', 'spiriting', 'country', 'frostier', 'treasury')\n",
            "government ('authority', 'participation', 'autonomy', 'failing', 'hawiye', 'extension', 'krg', 'partial', 'financing', 'sufficient')\n",
            "minister ('minster', 'ministerial', 'ministry', 'exponent', 'yue', 'kipp', 'theresamay', 'premier', 'bnd', 'drian')\n",
            "official ('source', 'diplomat', 'aide', 'anonymity', 'authority', 'zentaro', 'authorized', 'personnel', 'requested', 'separately')\n",
            "united ('gulf', 'islamic', 'reactivate', 'galmudug', 'koro', 'dreadful', 'isolated', 'vassal', 'duma', 'bessho')\n",
            "china ('beijing', 'chinese', 'taiwan', 'india', 'japan', 'vietnam', 'strait', 'bilateral', 'taipei', 'korea')\n",
            "north ('dpr', 'south', 'peninsula', 'resold', 'lettering', 'cha', 'ryonha', 'kogas', 'pyongyang', 'communicates')\n",
            "washington ('trump', 'moscow', 'braithwaite', 'riga', 'tillerson', 'warmer', 'oppositional', 'elect', 'windhoek', 'gips')\n",
            "party ('anc', 'ldp', 'fpo', 'corbyn', 'ano', 'spd', 'bloc', 'labour', 'renzi', 'afd')\n",
            "republican ('gop', 'rubio', 'cruz', 'ryan', 'bipartisan', 'repeal', 'flake', 'mcconnell', 'sander', 'senate')\n",
            "leader ('leadership', 'parliamentarian', 'colleague', 'politician', 'realign', 'lawmaker', 'counterpart', 'pdecat', 'phonecalls', 'unionist')\n",
            "korea ('korean', 'reclusive', 'maktab', 'carolinian', 'cotabato', 'cowshed', 'nonthaburi', 'pyongyang', 'ossetia', 'westphalia')\n",
            "tax ('amt', 'deduction', 'zip', 'expensing', 'income', 'rate', 'salt', 'corporate', 'repeal', 'inheritance')\n",
            "wednesday ('thursday', 'monday', 'tuesday', 'friday', 'saturday', 'sunday', 'week', 'august', 'september', 'july')\n",
            "house ('suffragette', 'supremacy', 'headteachers', 'cassock', 'supremacism', 'collar', 'impregnate', 'powder', 'congressional', 'dent')\n",
            "tuesday ('thursday', 'wednesday', 'monday', 'friday', 'saturday', 'sunday', 'week', 'september', 'august', 'oct')\n",
            "percent ('polled', 'quarter', 'compared', 'slightly', 'cent', 'gdp', 'overall', 'average', 'percentage', 'versus')\n",
            "senate ('congressional', 'congress', 'mcconnell', 'chamber', 'senator', 'filibuster', 'panel', 'representative', 'muster', 'legislature')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En general podemos apreciar que las noticias verdaderas usan palabras que mantienen un contexto más objetivo respecto a las falsas."
      ],
      "metadata": {
        "id": "sodId3g8FDh7"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}