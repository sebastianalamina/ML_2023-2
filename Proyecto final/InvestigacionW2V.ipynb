{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yj5OXfStHRAq"
   },
   "source": [
    "### Librerias Usadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "wbLhTObWvAwO",
    "outputId": "89fab7e9-49ba-4e8a-c8cb-1ec9941f7772"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tensorflow as tf\n",
    "import re, string, nltk\n",
    "from wordcloud import WordCloud\n",
    "# Descarga de recursos\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MkxUETQbS7xx",
    "outputId": "802bf158-e4d9-4fe4-e38e-929b2ddab2c5"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiZdPaiEOU5P"
   },
   "source": [
    "## Lectura de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DcuAIZKwf8K",
    "outputId": "6c59aeb0-3898-49e2-cee4-a13c1b324e28"
   },
   "outputs": [],
   "source": [
    "dataFake = pd.read_csv(\"/content/drive/MyDrive/Fake.csv\")\n",
    "dataFake[\"class\"] = 0\n",
    "print(\"Fake: \",dataFake.shape)\n",
    "\n",
    "dataTrue = pd.read_csv(\"/content/drive/MyDrive/True.csv\")\n",
    "dataTrue[\"class\"] = 1\n",
    "print(\"True: \",dataTrue.shape)\n",
    "\n",
    "data_merge = pd.concat([dataFake,dataTrue], axis=0)\n",
    "data = data_merge.drop([\"title\",\"subject\",\"date\"], axis=1)\n",
    "print(\"All data: \",data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2PeBL87OU5U"
   },
   "source": [
    "## Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLoc_nj33pBL"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  Función que elimina los símbolos especiales de un texto,\n",
    "  así como las stopwords\n",
    "'''\n",
    "def word_cleaner(text):\n",
    "  text = text.lower()\n",
    "  text = re.sub('\\[.*?\\]', '', text)\n",
    "  text = re.sub('\\\\W', ' ', text)\n",
    "  text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "  text = re.sub('<.*?>+', '', text)\n",
    "  text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "  text = re.sub('\\n', '', text)\n",
    "  text = re.sub('\\w*\\d\\w*', '', text)\n",
    "  return remove_stopwords(text)\n",
    "\n",
    "'''\n",
    "  Función que dado un texto, lo limpia y elimina las letras aisladas existentes.\n",
    "'''\n",
    "def text_cleaner(text, lemmatizer):\n",
    "  text = word_cleaner(text)\n",
    "  tokens = word_tokenize(text)\n",
    "  lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "  text = remove_letters(lemmatized_tokens)\n",
    "  text = \" \".join(text)\n",
    "  return text\n",
    "\n",
    "'''\n",
    "  Funcion que elimina las palabras con lengitud menor a length\n",
    "'''\n",
    "def remove_letters(lemas, length=2):\n",
    "  return [word for word in lemas if len(word)>length]\n",
    "\n",
    "'''\n",
    "  Funcion que cuenta la frecuencia de palabras en el dataset\n",
    "'''\n",
    "def count_tokens(texts, wf):\n",
    "  for text in texts:\n",
    "    tokens = text.split()\n",
    "    wf.update(tokens)\n",
    "  return wf\n",
    "\n",
    "'''\n",
    "  Funcion que dado el dataset y la lista de palabras que no tienen una \n",
    "  frecuencua valida las elimina del dataset\n",
    "'''\n",
    "def remove_max_min_words_freq(texts, words_to_remove):\n",
    "  filtered_texts = []\n",
    "  for text in texts:\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [word for word in tokens if word not in words_to_remove]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    filtered_texts.append(filtered_text)\n",
    "  return filtered_texts\n",
    "\n",
    "'''\n",
    "  Funcion que obtiene la lista de palabras a eliminar\n",
    "'''\n",
    "def get_words_to_remove(min_freq =2, max_freq=1000, word_freq=None):\n",
    "  return [word for word, freq in word_freq.items() if freq < min_freq or freq > max_freq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gráficas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud(word_list):\n",
    "\n",
    "    # Convert the list of words into a string\n",
    "    text = ' '.join(word_list)\n",
    "\n",
    "    # Create the word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "    # Display the word cloud using matplotlib\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def generate_pie_chart(data, title):\n",
    "    labels, porcentages = [], []\n",
    "    \n",
    "    for label, p in data:\n",
    "        labels.append(label)\n",
    "        porcentages.append(p)\n",
    "        \n",
    "    plt.pie(porcentages, labels=labels, autopct='%1.1f%%')\n",
    "    plt.axis('equal')  \n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vj6vfsFNIZWy"
   },
   "source": [
    "### Aplicación de limpieza a datos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0jFN8eZOU5V"
   },
   "outputs": [],
   "source": [
    "clean_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gw0EvHwy4lzZ"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "clean_data[\"text\"] = clean_data['text'].apply(text_cleaner, args=(lemmatizer,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wx-8gnumuMCq"
   },
   "outputs": [],
   "source": [
    "clean_fake = dataFake.copy()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "clean_fake['text'] = clean_fake['text'].apply(text_cleaner, args=(lemmatizer,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9UjrUA2w7XX"
   },
   "outputs": [],
   "source": [
    "clean_true = dataTrue.copy()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "clean_true['text'] = clean_true['text'].apply(text_cleaner, args=(lemmatizer,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJLFNJikOU5X"
   },
   "source": [
    "## Separación de datos de entrenamiento y de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWemUeuqOU5Y"
   },
   "outputs": [],
   "source": [
    "SEED = 123456789\n",
    "\n",
    "x = clean_data['text']\n",
    "y = clean_data['class']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=SEED)\n",
    "\n",
    "print(f\"Datos de entrenamiento: {len(x_train)} ({len(x_train)/len(x):%})\")\n",
    "print(f\"Datos de prueba: \\t{len(x_test)} ({len(x_test)/len(x):%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_BZHiivx6z0C",
    "outputId": "9b923664-ea06-4b1f-e2cd-a5ffca8de7e6"
   },
   "outputs": [],
   "source": [
    "SEED = 123456789\n",
    "\n",
    "x_true = clean_true['text']\n",
    "y_true = clean_true['class']\n",
    "\n",
    "x_train_true, x_test_true, y_train_true, y_test_true = train_test_split(x_true, y_true, test_size=0.25, random_state=SEED)\n",
    "\n",
    "print(f\"Datos de entrenamiento: {len(x_train_true)} ({len(x_train_true)/len(x_true):%})\")\n",
    "print(f\"Datos de prueba: \\t{len(x_test_true)} ({len(x_test_true)/len(x_true):%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22xMQgKU9f6x",
    "outputId": "9f983ed5-2050-484f-b686-9dd3981cea09"
   },
   "outputs": [],
   "source": [
    "x_fake = clean_fake['text']\n",
    "y_fake = clean_fake['class']\n",
    "\n",
    "x_train_fake, x_test_fake, y_train_fake, y_test_fake = train_test_split(x_fake, y_fake, test_size=0.25, random_state=SEED)\n",
    "\n",
    "print(f\"Datos de entrenamiento: {len(x_train_fake)} ({len(x_train_fake)/len(x_fake):%})\")\n",
    "print(f\"Datos de prueba: \\t{len(x_test_fake)} ({len(x_test_fake)/len(x_fake):%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YwspivV7Tln"
   },
   "source": [
    "### Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bpKM5FA7j21"
   },
   "outputs": [],
   "source": [
    "X = pd.concat([x_train_true, x_test_true], axis=0)\n",
    "sentences = [text.split() for text in X]\n",
    "word2vec_model = Word2Vec(sentences, min_count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25 palabras más comunes en True.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOoakMrP8V-L"
   },
   "outputs": [],
   "source": [
    "generate_word_cloud(word2vec_model.wv.index_to_key[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8bq4Bpo9cnw"
   },
   "outputs": [],
   "source": [
    "X_fake = pd.concat([x_train_fake, x_test_fake], axis=0)\n",
    "sentences_fake = [text.split() for text in X_fake]\n",
    "word2vec_model_f = Word2Vec(sentences_fake, min_count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25 palabras más comunes en Fake.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_cloud(word2vec_model_f.wv.index_to_key[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flFsySEoFd4J"
   },
   "source": [
    "Usando la función `most_similar`, buscamos para cada modelo las palabras relacionadas a una de las que más repeticiones tuvieron (tanto en las noticias falsas como las verdaderas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oNx6ajLZ8h1U",
    "outputId": "0d04c858-b9e7-477f-981f-a7df755877fa"
   },
   "outputs": [],
   "source": [
    "word2vec_model.wv.most_similar('trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_trump_words = word2vec_model.wv.most_similar('trump')\n",
    "generate_pie_chart(true_trump_words, \"Palabras asociadas a Trump en True.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bqjDDDts-xpm",
    "outputId": "1ee666c4-7a18-49f9-9b6c-84475b1421fc"
   },
   "outputs": [],
   "source": [
    "word2vec_model_f.wv.most_similar('trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_trump_words = word2vec_model_f.wv.most_similar('trump')\n",
    "generate_pie_chart(fake_trump_words, \"Palabras asociadas a Trump en Fake.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aBdBd1hR2K7f",
    "outputId": "009c830c-5040-4bc3-d52c-25face089d25"
   },
   "outputs": [],
   "source": [
    "list(zip(*word2vec_model_f.wv.most_similar('trump')))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xE114qVjG3zq"
   },
   "source": [
    "Ahora, apoyados de los resultados obtenidos en la investigación de `CountVectorizer`, se obtendrán las palabras relacionadas para cada una de las que más aparecieron en cada tipo de noticias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UVKa6Y6yHmj"
   },
   "outputs": [],
   "source": [
    "fake_keywords = [\n",
    "    'medium', 'donald', 'black', 'video',\n",
    "    'woman', 'com', 'featured', 'news', \n",
    "    'america', 'twitter', 'obama', 'time',\n",
    "    'know', 'clinton', 'american', 'people',\n",
    "    'hillary', 'like', 'image', 'trump'\n",
    "]\n",
    "\n",
    "true_keywords = [\n",
    "    'said', 'reuters', 'state', 'government',\n",
    "    'minister', 'official', 'united', 'china',\n",
    "    'north', 'washington', 'party', 'republican',\n",
    "    'leader', 'korea', 'tax', 'wednesday', \n",
    "    'house', 'tuesday', 'percent', 'senate',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j6mG5kFw0R_o",
    "outputId": "1195d6fb-be16-48b3-ffac-165a38bffaff"
   },
   "outputs": [],
   "source": [
    "fake = dict()\n",
    "for word in fake_keywords:\n",
    "  #r.append(list(zip(*word2vec_model_f.wv.most_similar(word)))[0])\n",
    "  fake[word] =  list(zip(*word2vec_model_f.wv.most_similar(word)))[0]\n",
    "  print(word, list(zip(*word2vec_model_f.wv.most_similar(word)))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eTC9YZODE5M0",
    "outputId": "f75caf48-6a98-4625-d2b1-609fb73fccb2"
   },
   "outputs": [],
   "source": [
    "true = dict()\n",
    "for word in true_keywords:\n",
    "  #r.append(list(zip(*word2vec_model_f.wv.most_similar(word)))[0])\n",
    "  true[word] =  list(zip(*word2vec_model.wv.most_similar(word)))[0]\n",
    "  print(word, list(zip(*word2vec_model.wv.most_similar(word)))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sodId3g8FDh7"
   },
   "source": [
    "En general podemos apreciar que las noticias verdaderas usan palabras que mantienen un contexto más objetivo respecto a las falsas."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
