{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yj5OXfStHRAq"
   },
   "source": [
    "### Librerias Usadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbLhTObWvAwO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tensorflow as tf\n",
    "import re, string, nltk\n",
    "# Descarga de recursos\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiZdPaiEOU5P"
   },
   "source": [
    "## Lectura de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_DcuAIZKwf8K"
   },
   "outputs": [],
   "source": [
    "dataFake = pd.read_csv(\"Fake.csv\")\n",
    "dataFake[\"class\"] = 0\n",
    "print(\"Fake: \",dataFake.shape)\n",
    "\n",
    "dataTrue = pd.read_csv(\"True.csv\")\n",
    "dataTrue[\"class\"] = 1\n",
    "print(\"True: \",dataTrue.shape)\n",
    "\n",
    "data_merge = pd.concat([dataFake,dataTrue], axis=0)\n",
    "data = data_merge.drop([\"title\",\"subject\",\"date\"], axis=1)\n",
    "print(\"All data: \",data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2PeBL87OU5U"
   },
   "source": [
    "## Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JLoc_nj33pBL"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "  Función que elimina los símbolos especiales de un texto,\n",
    "  así como las stopwords\n",
    "'''\n",
    "def word_cleaner(text):\n",
    "  text = text.lower()\n",
    "  text = re.sub('\\[.*?\\]', '', text)\n",
    "  text = re.sub('\\\\W', ' ', text)\n",
    "  text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "  text = re.sub('<.*?>+', '', text)\n",
    "  text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "  text = re.sub('\\n', '', text)\n",
    "  text = re.sub('\\w*\\d\\w*', '', text)\n",
    "  return remove_stopwords(text)\n",
    "\n",
    "'''\n",
    "  Función que dado un texto, lo limpia y elimina las letras aisladas existentes.\n",
    "'''\n",
    "def text_cleaner(text, lemmatizer):\n",
    "  text = word_cleaner(text)\n",
    "  tokens = word_tokenize(text)\n",
    "  lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "  text = remove_letters(lemmatized_tokens)\n",
    "  text = \" \".join(text)\n",
    "  return text\n",
    "\n",
    "'''\n",
    "  Funcion que elimina las palabras con lengitud menor a length\n",
    "'''\n",
    "def remove_letters(lemas, length=2):\n",
    "  return [word for word in lemas if len(word)>length]\n",
    "\n",
    "'''\n",
    "  Funcion que cuenta la frecuencia de palabras en el dataset\n",
    "'''\n",
    "def count_tokens(texts, wf):\n",
    "  for text in texts:\n",
    "    tokens = text.split()\n",
    "    wf.update(tokens)\n",
    "  return wf\n",
    "\n",
    "'''\n",
    "  Funcion que dado el dataset y la lista de palabras que no tienen una \n",
    "  frecuencua valida las elimina del dataset\n",
    "'''\n",
    "def remove_max_min_words_freq(texts, words_to_remove):\n",
    "  filtered_texts = []\n",
    "  for text in texts:\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [word for word in tokens if word not in words_to_remove]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    filtered_texts.append(filtered_text)\n",
    "  return filtered_texts\n",
    "\n",
    "'''\n",
    "  Funcion que obtiene la lista de palabras a eliminar\n",
    "'''\n",
    "def get_words_to_remove(min_freq =2, max_freq=1000, word_freq=None):\n",
    "  return [word for word, freq in word_freq.items() if freq < min_freq or freq > max_freq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vj6vfsFNIZWy"
   },
   "source": [
    "### Aplicación de limpieza a datos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "X0jFN8eZOU5V"
   },
   "outputs": [],
   "source": [
    "clean_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Gw0EvHwy4lzZ"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "clean_data[\"text\"] = clean_data['text'].apply(text_cleaner, args=(lemmatizer,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XWvRdH7IpiL"
   },
   "source": [
    "### Solo es requerido ejecutar esta celsa si se desea probar la eliminación de palabras con frecuencia max y min.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4mV0gCef0G9"
   },
   "outputs": [],
   "source": [
    "# Importacion de recursos\n",
    "#from collections import Counter\n",
    "# Instancias \n",
    "#word_freq = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1W0A2YLV5oYN"
   },
   "outputs": [],
   "source": [
    "# head = clean_data.head(100)\n",
    "# tail = clean_data.tail(100)\n",
    "# clean_data = pd.concat([head,tail])\n",
    "# word_freq = count_tokens(clean_data['text'], word_freq)\n",
    "# words = get_words_to_remove(5,1000,word_freq)\n",
    "# clean_data[\"text\"] = remove_max_min_words_freq(clean_data[\"text\"] ,words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJLFNJikOU5X"
   },
   "source": [
    "## Separación de datos de entrenamiento y de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWemUeuqOU5Y"
   },
   "outputs": [],
   "source": [
    "SEED = 123456789\n",
    "\n",
    "x = clean_data['text']\n",
    "y = clean_data['class']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=SEED)\n",
    "\n",
    "print(f\"Datos de entrenamiento: {len(x_train)} ({len(x_train)/len(x):%})\")\n",
    "print(f\"Datos de prueba: \\t{len(x_test)} ({len(x_test)/len(x):%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVgwv7c0OU5Z"
   },
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q4iw7H-OU5Z"
   },
   "source": [
    "### Vectorización TFID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Odm2jw3IOU5a"
   },
   "outputs": [],
   "source": [
    "tfid_vectorizer = TfidfVectorizer()\n",
    "x_tfid_train = tfid_vectorizer.fit_transform(x_train)\n",
    "x_tfid_test = tfid_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hvTcFTyOU5c"
   },
   "source": [
    "### Vectorización por frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9vIDe8IWOU5c"
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "x_count_train = count_vectorizer.fit_transform(x_train)\n",
    "x_count_test = count_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YwspivV7Tln"
   },
   "source": [
    "### Vectorización por Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kcX3fpgj9hKT"
   },
   "outputs": [],
   "source": [
    "# Vectorización con Word2Vec\n",
    "def vectorize_W2V(x_train, x_test):\n",
    "  # Concatenar series2 al frente de series1\n",
    "  X = pd.concat([x_train, x_test], axis=0)\n",
    "  sentences = [text.split() for text in X]\n",
    "  word2vec_model = Word2Vec(sentences, min_count=1)\n",
    "  # Obtener el tamaño de los vectores de Word2Vec\n",
    "  embedding_size = word2vec_model.vector_size\n",
    "  # Obtener el promedio de los vectores de Word2Vec para cada texto\n",
    "  X_word2vec = np.zeros((len(sentences), embedding_size))\n",
    "  for i, text in enumerate(sentences):\n",
    "      count = 0\n",
    "      for word in text:\n",
    "          if word in word2vec_model.wv:\n",
    "              X_word2vec[i] += word2vec_model.wv[word]\n",
    "              count += 1\n",
    "      if count > 0:\n",
    "          X_word2vec[i] /= count\n",
    "  return X_word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2y8jcliOU5e"
   },
   "source": [
    "## Reducción de dimensiones con SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvaUTHfbOU5e"
   },
   "source": [
    "Dado que KNN es un algoritmo ineficiente para vectores grandes, se generan versiones reducidas en dimensión para cada vectorización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3_yI6zxsOU5e"
   },
   "outputs": [],
   "source": [
    "N_COMPONENTS = 800 # Arbitrario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pj3-HcurOU5f"
   },
   "outputs": [],
   "source": [
    "svd_tfid = TruncatedSVD(n_components=N_COMPONENTS)\n",
    "x_tfid_train_svd = svd_tfid.fit_transform(x_tfid_train)\n",
    "x_tfid_test_svd = svd_tfid.transform(x_tfid_test)\n",
    "print(f\"Varianza preservada: {svd_tfid.explained_variance_ratio_.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Niz8Y6nOU5f"
   },
   "outputs": [],
   "source": [
    "svd_count = TruncatedSVD(n_components=N_COMPONENTS)\n",
    "x_count_train_svd = svd_count.fit_transform(x_count_train)\n",
    "x_count_test_svd = svd_count.transform(x_count_test)\n",
    "print(f\"Varianza preservada: {svd_count.explained_variance_ratio_.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luo5TClkOU5g"
   },
   "source": [
    "## Ajuste de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKKl-uCXOU5g"
   },
   "source": [
    "### Regresión logística sin SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auQDHQsdOU5g"
   },
   "source": [
    "Se ajusta un modelo de regresión logística a los vectores generados por TFID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0Esql-47WCe"
   },
   "outputs": [],
   "source": [
    "LR_tfid = LogisticRegression(max_iter=1000)\n",
    "LR_tfid.fit(x_tfid_train, y_train)\n",
    "print(f\"El algoritmo convergió después de {LR_tfid.n_iter_} iteraciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJCWM3F0OU5i"
   },
   "source": [
    "Se ajusta un modelo de regresión logística a los vectores generados por frecuencias de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxtlMqaKOU5i"
   },
   "outputs": [],
   "source": [
    "LR_count = LogisticRegression(max_iter=2000)\n",
    "LR_count.fit(x_count_train, y_train)\n",
    "print(f\"El algoritmo convergió después de {LR_count.n_iter_} iteraciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9K65l0SHOU5j"
   },
   "source": [
    "### Regresión Logísitca con SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dARD0sGZOU5j"
   },
   "source": [
    "Se ajusta un modelo de regresión logística a los vectores generados por TFID y procesados por SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abp8fZeKOU5k"
   },
   "outputs": [],
   "source": [
    "LR_tfid_svd = LogisticRegression(max_iter=1000)\n",
    "LR_tfid_svd.fit(x_tfid_train_svd, y_train)\n",
    "print(f\"El algoritmo convergió después de {LR_tfid_svd.n_iter_} iteraciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPMMGZ2oOU5k"
   },
   "source": [
    "Se ajusta un modelo de regresión logística a los vectores generados por frecuencias de palabras y procesados por SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lICldcWHOU5l"
   },
   "outputs": [],
   "source": [
    "LR_count_svd = LogisticRegression(max_iter=2000)\n",
    "LR_count_svd.fit(x_count_train_svd, y_train)\n",
    "print(f\"El algoritmo convergió después de {LR_count_svd.n_iter_} iteraciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOo1lvrzOU5m"
   },
   "source": [
    "### K Nearest Neighbors con SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vpd2Zb3lOU5m"
   },
   "source": [
    "Se obtiene un valor de $K$ sensato para los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "u22Xbq2IOU5n"
   },
   "outputs": [],
   "source": [
    "K = 50 # Arbitrario\n",
    "N_JOBS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rAQ_jiUOU5n"
   },
   "source": [
    "Se ajusta un modelo KNN a los vectores generados por TFID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDNSC6F3-gjY"
   },
   "outputs": [],
   "source": [
    "KNN_tfid_svd = KNeighborsClassifier(n_neighbors=K, n_jobs=N_JOBS)\n",
    "KNN_tfid_svd.fit(x_tfid_train_svd, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FW8POwKUOU5o"
   },
   "source": [
    "Se ajusta un modelo KNN a los vectores generados por frecuencias de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ezL96pzmOU6C"
   },
   "outputs": [],
   "source": [
    "KNN_count_svd = KNeighborsClassifier(n_neighbors=K, n_jobs=N_JOBS)\n",
    "KNN_count_svd.fit(x_count_train_svd, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXsteMr7OU6D"
   },
   "source": [
    "### Evaluación de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "bID9JiDFOU6E"
   },
   "outputs": [],
   "source": [
    "class TrainedModel:\n",
    "    def __init__(self, model, name, x_test, y_test) -> None:\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.predict = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Uedsvm7TOU6F"
   },
   "outputs": [],
   "source": [
    "LR_tfid_tm = TrainedModel(LR_tfid, \"LR + TFID\", x_tfid_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Re5ApnyGOU6G"
   },
   "outputs": [],
   "source": [
    "LR_count_tm = TrainedModel(LR_count, \"LR + Count\", x_count_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "JwE5VHeLOU6H"
   },
   "outputs": [],
   "source": [
    "LR_tfid_svd_tm = TrainedModel(LR_tfid_svd, f\"LR + TFID + SVD ({N_COMPONENTS})\", x_tfid_test_svd, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "QZe6r8DhOU6H"
   },
   "outputs": [],
   "source": [
    "LR_count_svd_tm = TrainedModel(LR_count_svd, f\"LR + Count + SVD ({N_COMPONENTS})\", x_count_test_svd, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "2x9GeMKoOU6H"
   },
   "outputs": [],
   "source": [
    "KNN_tfid_svd_tm = TrainedModel(KNN_tfid_svd, f\"KNN ({KNN_tfid_svd.n_neighbors}) + TFID + SVD ({N_COMPONENTS})\", x_tfid_test_svd, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "vbXgoz0yOU6I"
   },
   "outputs": [],
   "source": [
    "KNN_count_svd_tm = TrainedModel(KNN_count_svd, f\"KNN ({KNN_count_svd.n_neighbors}) + Count + SVD ({N_COMPONENTS})\", x_count_test_svd, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "4K_wILvPOU6I"
   },
   "outputs": [],
   "source": [
    "trained_models = [\n",
    "    LR_tfid_tm, LR_count_tm, LR_tfid_svd_tm, LR_count_svd_tm, KNN_tfid_svd_tm, KNN_count_svd_tm\n",
    "]\n",
    "labels = [0, 1]\n",
    "target_names = [\"Fake\", \"True\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S18_sxkROU6J"
   },
   "source": [
    "### Reportes de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtfNxOjOOU6J"
   },
   "outputs": [],
   "source": [
    "def plot_clasification_reports(trained_models, labels, target_names, cols=2):\n",
    "    # Se generan los reportes\n",
    "    reports = [\n",
    "        pd.DataFrame(\n",
    "            classification_report(\n",
    "                m.y_test,\n",
    "                m.predict,\n",
    "                labels=labels,\n",
    "                target_names=target_names,\n",
    "                output_dict=True,\n",
    "            )\n",
    "        )\n",
    "        for m in trained_models\n",
    "    ]\n",
    "\n",
    "    # Se obtiene la norma de colores de todos los modelos\n",
    "    # https://stackoverflow.com/a/70517313/15217078\n",
    "    values = np.hstack([d.iloc[:-1, :].values.ravel() for d in reports])\n",
    "    norm = mcolors.Normalize(values.min(), values.max())\n",
    "\n",
    "    # Se generan las gráficas de los reportes\n",
    "    # # https://stackoverflow.com/a/58948133/15217078\n",
    "    rows = int(np.ceil(len(trained_models) / cols))\n",
    "    fig, axes = plt.subplots(rows, cols)\n",
    "    fig.set_size_inches(8 * cols, 5 * rows)\n",
    "    for i, m in enumerate(trained_models):\n",
    "        ax = axes[i // cols][i % cols]\n",
    "        r = reports[i]\n",
    "        sns.heatmap(r.iloc[:-1, :].T, annot=True, norm=norm, ax=ax)\n",
    "        ax.set_title(m.name)\n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "report_fig, report_axes = plot_clasification_reports(\n",
    "    trained_models, labels, target_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwM1QT2WI8iB"
   },
   "source": [
    "### Composición de vectorizaciones (Word2Vec y CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "-Yvt9Yn9jE5J"
   },
   "outputs": [],
   "source": [
    "# Obtenemos la vectorización de los conjuntos por medio de Count\n",
    "cv = CountVectorizer()\n",
    "x_count_train = cv.fit_transform(x_train)\n",
    "x_count_test = cv.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "67PUWkA0l0tL"
   },
   "outputs": [],
   "source": [
    "# Vectorización con Word2Vec\n",
    "X_word2vec = vectorize_W2V(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "LAfen31BB7jS"
   },
   "outputs": [],
   "source": [
    "# Obtenemos los conjutos ya vectorizados\n",
    "X_word2vec_train = X_word2vec[:len(x_train)]\n",
    "X_word2vec_test = X_word2vec[len(x_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "csRNpAIRbMXc"
   },
   "outputs": [],
   "source": [
    "# Concatenamos las matrices dispersas y los arreglos a lo largo del eje de las columnas\n",
    "X_concatenated_train = hstack([x_count_train, X_word2vec_train])\n",
    "X_concatenated_test = hstack([x_count_test, X_word2vec_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlLQ9vP8jQmT"
   },
   "outputs": [],
   "source": [
    "# Entrenar el modelo de regresión logística\n",
    "logreg = LogisticRegression(max_iter=2000)\n",
    "logreg.fit(X_concatenated_train, y_train)\n",
    "print(f\"El algoritmo convergió después de {logreg.n_iter_} iteraciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "KUMoEbuSjShl"
   },
   "outputs": [],
   "source": [
    "# Predecir las etiquetas para los datos de prueba\n",
    "y_pred = logreg.predict(X_concatenated_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jw3PCg2jjUGz"
   },
   "outputs": [],
   "source": [
    "# Calcular la precisión del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisión del modelo:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_u1q5AAp5gW9"
   },
   "outputs": [],
   "source": [
    "# Reducción de dimensionalidad para X_count\n",
    "N_COMPONENTS = 100\n",
    "svd_composition = TruncatedSVD(n_components=N_COMPONENTS)\n",
    "x_comp_train_svd = svd_composition.fit_transform(x_count_train)\n",
    "x_comp_test_svd = svd_composition.transform(x_count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "5WAkxlsC3F8d"
   },
   "outputs": [],
   "source": [
    "# Concatenamos las matrices dispersas y los arreglos a lo largo del eje de las columnas\n",
    "X_concatenated_train_svd= np.concatenate((x_comp_train_svd, X_word2vec_train), axis=1)\n",
    "X_concatenated_test_svd= np.concatenate((x_comp_test_svd, X_word2vec_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czsvewZnMDV7"
   },
   "outputs": [],
   "source": [
    "# Entrenar el modelo de regresión logística\n",
    "logreg_svd = LogisticRegression(max_iter=2000)\n",
    "logreg_svd.fit(X_concatenated_train_svd, y_train)\n",
    "print(f\"El algoritmo convergió después de {logreg_svd.n_iter_} iteraciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "2kESFKWWMJCY"
   },
   "outputs": [],
   "source": [
    "# Predecir las etiquetas para los datos de prueba\n",
    "y_pred = logreg_svd.predict(X_concatenated_test_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9trGLdrMMgS"
   },
   "outputs": [],
   "source": [
    "# Calcular la precisión del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Precisión del modelo (SVD sobre Count a {N_COMPONENTS}):\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZl2eC7ibvdC"
   },
   "outputs": [],
   "source": [
    "# Creamos los modelos creados anteriormente\n",
    "LR_Word_Count = TrainedModel(logreg, \"LR + (Word2Vec & Count)\", X_concatenated_test, y_test)\n",
    "LR_Word_Count_SVD = TrainedModel(logreg_svd, \"LR + (Word2Vec & Count & SVD)\", X_concatenated_test_svd, y_test)\n",
    "# Los añadimos a la lista de modelos entrenados\n",
    "trained_models.append(LR_Word_Count)\n",
    "trained_models.append(LR_Word_Count_SVD)\n",
    "\n",
    "report_fig, report_axes = plot_clasification_reports(\n",
    "    trained_models, labels, target_names\n",
    ")\n",
    "report_fig, report_axes = plot_clasification_reports(trained_models, labels, target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se prueban diferentes valores para K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_values = [\n",
    "    75,\n",
    "    100,\n",
    "    150,\n",
    "    200,\n",
    "    225,\n",
    "    250,\n",
    "    300,\n",
    "    350,\n",
    "    400,\n",
    "    500,\n",
    "    600,\n",
    "    700\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primero probamos con TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_TFID_models = []\n",
    "N_COMPONENTS = 800\n",
    "\n",
    "for k in K_values:\n",
    "    model = KNeighborsClassifier(n_neighbors=k, n_jobs=N_JOBS)\n",
    "    model.fit(x_tfid_train_svd, y_train)\n",
    "    KNN_TFID_models.append(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_KNN_TDIF_models = []\n",
    "for model in KNN_TFID_models:\n",
    "    trained_model = TrainedModel(model, f\"KNN ({model.n_neighbors}) + TFID + SVD ({N_COMPONENTS})\", x_tfid_test_svd, y_test)\n",
    "    trained_KNN_TDIF_models.append(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_fig, report_axes = plot_clasification_reports(\n",
    "    trained_KNN_TDIF_models, labels, target_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ahora probamos con CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_COUNT_models = []\n",
    "N_COMPONENTS = 800\n",
    "\n",
    "for k in K_values:\n",
    "    model = KNeighborsClassifier(n_neighbors=k, n_jobs=N_JOBS)\n",
    "    model.fit(x_count_train_svd, y_train)\n",
    "    KNN_COUNT_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_KNN_COUNT_models = []\n",
    "for model in KNN_TFID_models:\n",
    "    trained_model = TrainedModel(model, f\"KNN ({model.n_neighbors}) + COUNT + SVD ({N_COMPONENTS})\", x_tfid_test_svd, y_test)\n",
    "    trained_KNN_COUNT_models.append(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_fig, report_axes = plot_clasification_reports(\n",
    "    trained_KNN_COUNT_models, labels, target_names\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
