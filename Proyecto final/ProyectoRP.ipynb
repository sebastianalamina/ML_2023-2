{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "wbLhTObWvAwO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import re, string\n",
        "from gensim.parsing.preprocessing import remove_stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiZdPaiEOU5P"
      },
      "source": [
        "## Lectura de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DcuAIZKwf8K"
      },
      "outputs": [],
      "source": [
        "dataFake = pd.read_csv(\"Fake.csv\")\n",
        "dataFake[\"class\"]=0\n",
        "print(\"Fake:\\n\",dataFake.shape)\n",
        "\n",
        "dataTrue = pd.read_csv(\"True.csv\")\n",
        "dataTrue[\"class\"]=1\n",
        "print(\"True:\\n\",dataTrue.shape)\n",
        "\n",
        "data_merge = pd.concat([dataFake,dataTrue], axis=0)\n",
        "data = data_merge.drop([\"title\",\"subject\",\"date\"], axis=1)\n",
        "print(\"All data:\\n\",data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2PeBL87OU5U"
      },
      "source": [
        "## Limpieza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "JLoc_nj33pBL"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "  Función que elimina los símbolos especiales de un texto,\n",
        "  así como las stopwords\n",
        "'''\n",
        "def word_cleaner(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub('\\[.*?\\]', '', text)\n",
        "  text = re.sub('\\\\W', ' ', text)\n",
        "  text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "  text = re.sub('<.*?>+', '', text)\n",
        "  text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "  text = re.sub('\\n', '', text)\n",
        "  text = re.sub('\\w*\\d\\w*', '', text)\n",
        "  return remove_stopwords(text)\n",
        "\n",
        "'''\n",
        "  Función que dado un texto, lo limpia y elimina las letras aisladas existentes.\n",
        "'''\n",
        "def text_cleaner(text, lemmatizer):\n",
        "  text = word_cleaner(text)\n",
        "  tokens = word_tokenize(text)\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "  text = remove_letters(lemmatized_tokens)\n",
        "  text = \" \".join(text)\n",
        "  return text\n",
        "\n",
        "'''\n",
        "  Funcion que elimina las palabras con lengitud menor a length\n",
        "'''\n",
        "def remove_letters(lemas, length=2):\n",
        "  return [word for word in lemas if len(word)>length]\n",
        "\n",
        "'''\n",
        "  Funcion que cuenta la frecuencia de palabras en el dataset\n",
        "'''\n",
        "def count_tokens(texts, wf):\n",
        "  for text in texts:\n",
        "    tokens = text.split()\n",
        "    wf.update(tokens)\n",
        "  return wf\n",
        "\n",
        "'''\n",
        "  Funcion que dado el dataset y la lista de palabras que no tienen una \n",
        "  frecuencua valida las elimina del dataset\n",
        "'''\n",
        "def remove_max_min_words_freq(texts, words_to_remove):\n",
        "  filtered_texts = []\n",
        "  for text in texts:\n",
        "    tokens = text.split()\n",
        "    filtered_tokens = [word for word in tokens if word not in words_to_remove]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    filtered_texts.append(filtered_text)\n",
        "  return filtered_texts\n",
        "\n",
        "'''\n",
        "  Funcion que obtiene la lista de palabras a eliminar\n",
        "'''\n",
        "def get_words_to_remove(min_freq =2, max_freq=1000, word_freq=None):\n",
        "  return [word for word, freq in word_freq.items() if freq < min_freq or freq > max_freq]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importaciones para probar la eliminación de palabras con máxima y mínima frecuencia.\n"
      ],
      "metadata": {
        "id": "vj6vfsFNIZWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import nltk\n",
        "# Descarga de recursos\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "# Instancias \n",
        "word_freq = Counter()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "R4mV0gCef0G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "X0jFN8eZOU5V"
      },
      "outputs": [],
      "source": [
        "clean_data = data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_data[\"text\"] = clean_data['text'].apply(text_cleaner,args=(lemmatizer,))"
      ],
      "metadata": {
        "id": "Gw0EvHwy4lzZ"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solo es requerido ejecutar esta celsa si se desea probar la eliminación de palabras con frecuencia max y min.\n"
      ],
      "metadata": {
        "id": "0XWvRdH7IpiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "head = clean_data.head(100)\n",
        "tail = clean_data.tail(100)\n",
        "clean_data = pd.concat([head,tail])\n",
        "word_freq = count_tokens(clean_data['text'], word_freq)\n",
        "words = get_words_to_remove(5,1000,word_freq)\n",
        "clean_data[\"text\"] = remove_max_min_words_freq(clean_data[\"text\"] ,words)"
      ],
      "metadata": {
        "id": "1W0A2YLV5oYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJLFNJikOU5X"
      },
      "source": [
        "## Separación de datos de entrenamiento y de prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWemUeuqOU5Y"
      },
      "outputs": [],
      "source": [
        "SEED = 123456789\n",
        "\n",
        "x = clean_data['text']\n",
        "y = clean_data['class']\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.25, random_state=SEED)\n",
        "\n",
        "print(f\"Datos de entrenamiento: {len(x_train)} ({len(x_train)/len(x):%})\")\n",
        "print(f\"Datos de prueba: \\t{len(x_test)} ({len(x_test)/len(x):%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVgwv7c0OU5Z"
      },
      "source": [
        "## Preprocesamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q4iw7H-OU5Z"
      },
      "source": [
        "### Vectorización TFID:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Odm2jw3IOU5a"
      },
      "outputs": [],
      "source": [
        "tfid_vectorizer = TfidfVectorizer()\n",
        "def vectorize_TFID(x, mode=\"test\"):\n",
        "    if mode == \"train\":\n",
        "        return tfid_vectorizer.fit_transform(x)\n",
        "    elif mode == \"test\":\n",
        "        return tfid_vectorizer.transform(x)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A23Ys86wOU5b"
      },
      "outputs": [],
      "source": [
        "x_tfid_train = vectorize_TFID(x_train, mode=\"train\")\n",
        "x_tfid_test = vectorize_TFID(x_test, mode=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hvTcFTyOU5c"
      },
      "source": [
        "### Vectorización por frecuencia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vIDe8IWOU5c"
      },
      "outputs": [],
      "source": [
        "count_vectorizer = CountVectorizer()\n",
        "def vectorize_Count(x, mode=\"test\"):\n",
        "    if mode == \"train\":\n",
        "        return count_vectorizer.fit_transform(x)\n",
        "    elif mode == \"test\":\n",
        "        return count_vectorizer.transform(x)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TWsYx6aOU5d"
      },
      "outputs": [],
      "source": [
        "x_count_train = vectorize_Count(x_train, mode=\"train\")\n",
        "x_count_test = vectorize_Count(x_test, mode=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filas, col = x_count_train.shape\n",
        "print(filas)\n",
        "print(col)"
      ],
      "metadata": {
        "id": "kTEKu5is6wf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2y8jcliOU5e"
      },
      "source": [
        "## Reducción de dimensiones con PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvaUTHfbOU5e"
      },
      "source": [
        "Dado que KNN es un algoritmo ineficiente para vectores grandes, se generan versiones reducidas en dimensión para cada vectorización."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_yI6zxsOU5e"
      },
      "outputs": [],
      "source": [
        "N_COMPONENTS = 500 # FIXME encontrar mejor valor de N_COMPONENTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj3-HcurOU5f"
      },
      "outputs": [],
      "source": [
        "svd_tfid = TruncatedSVD(n_components=N_COMPONENTS)\n",
        "x_tfid_train_svd = svd_tfid.fit_transform(x_tfid_train)\n",
        "x_tfid_test_svd = svd_tfid.transform(x_tfid_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Niz8Y6nOU5f"
      },
      "outputs": [],
      "source": [
        "svd_count = TruncatedSVD(n_components=N_COMPONENTS)\n",
        "x_count_train_svd = svd_count.fit_transform(x_count_train)\n",
        "x_count_test_svd = svd_count.transform(x_count_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luo5TClkOU5g"
      },
      "source": [
        "## Ajuste de modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKKl-uCXOU5g"
      },
      "source": [
        "### Regresión logística sin SVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auQDHQsdOU5g"
      },
      "source": [
        "Se ajusta un modelo de regresión logística a los vectores generados por TFID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0Esql-47WCe"
      },
      "outputs": [],
      "source": [
        "LR_tfid = LogisticRegression(max_iter=1000)\n",
        "LR_tfid.fit(x_tfid_train, y_train)\n",
        "print(f\"El algoritmo convergió después de {LR_tfid.n_iter_} iteraciones\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJCWM3F0OU5i"
      },
      "source": [
        "Se ajusta un modelo de regresión logística a los vectores generados por frecuencias de palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxtlMqaKOU5i",
        "outputId": "52173bcd-b2bf-483b-82cd-52351d2b0168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El algoritmo convergió después de [132] iteraciones\n"
          ]
        }
      ],
      "source": [
        "LR_count = LogisticRegression(max_iter=2000)\n",
        "LR_count.fit(x_count_train, y_train)\n",
        "print(f\"El algoritmo convergió después de {LR_count.n_iter_} iteraciones\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K65l0SHOU5j"
      },
      "source": [
        "### Regresión Logísitca con SVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dARD0sGZOU5j"
      },
      "source": [
        "Se ajusta un modelo de regresión logística a los vectores generados por TFID y procesados por SVD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abp8fZeKOU5k",
        "outputId": "46bb41f9-877c-481f-92c2-10b600a1bae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El algoritmo convergió después de [34] iteraciones\n"
          ]
        }
      ],
      "source": [
        "LR_tfid_svd = LogisticRegression(max_iter=1000)\n",
        "LR_tfid_svd.fit(x_tfid_train_svd, y_train)\n",
        "print(f\"El algoritmo convergió después de {LR_tfid_svd.n_iter_} iteraciones\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPMMGZ2oOU5k"
      },
      "source": [
        "Se ajusta un modelo de regresión logística a los vectores generados por frecuencias de palabras y procesados por SVD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lICldcWHOU5l",
        "outputId": "2aba792b-dc95-4c49-c8b8-cbc7a4e38982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El algoritmo convergió después de [202] iteraciones\n"
          ]
        }
      ],
      "source": [
        "LR_count_svd = LogisticRegression(max_iter=2000)\n",
        "LR_count_svd.fit(x_count_train_svd, y_train)\n",
        "print(f\"El algoritmo convergió después de {LR_count_svd.n_iter_} iteraciones\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOo1lvrzOU5m"
      },
      "source": [
        "### K Nearest Neighbors con SVD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vpd2Zb3lOU5m"
      },
      "source": [
        "Se obtiene un valor de $K$ sensato para los modelos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u22Xbq2IOU5n"
      },
      "outputs": [],
      "source": [
        "K = 5 # FIXME Encontrar mejor valor de K\n",
        "N_JOBS = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rAQ_jiUOU5n"
      },
      "source": [
        "Se ajusta un modelo KNN a los vectores generados por TFID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDNSC6F3-gjY"
      },
      "outputs": [],
      "source": [
        "KNN_tfid_svd = KNeighborsClassifier(n_neighbors=K, n_jobs=N_JOBS)\n",
        "KNN_tfid_svd.fit(x_tfid_train_svd, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW8POwKUOU5o"
      },
      "source": [
        "Se ajusta un modelo KNN a los vectores generados por frecuencias de palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezL96pzmOU6C"
      },
      "outputs": [],
      "source": [
        "KNN_count_svd = KNeighborsClassifier(n_neighbors=K, n_jobs=N_JOBS)\n",
        "KNN_count_svd.fit(x_count_train_svd, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXsteMr7OU6D"
      },
      "source": [
        "## Evaluación de los modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bID9JiDFOU6E"
      },
      "outputs": [],
      "source": [
        "class TrainedModel:\n",
        "    def __init__(self, model, name, x_test, y_test) -> None:\n",
        "        self.model = model\n",
        "        self.name = name\n",
        "        self.x_test = x_test\n",
        "        self.y_test = y_test\n",
        "        self.predict = model.predict(x_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uedsvm7TOU6F"
      },
      "outputs": [],
      "source": [
        "LR_tfid_tm = TrainedModel(LR_tfid, \"LR + TFID\", x_tfid_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Re5ApnyGOU6G"
      },
      "outputs": [],
      "source": [
        "LR_count_tm = TrainedModel(LR_tfid, \"LR + Count\", x_count_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwE5VHeLOU6H"
      },
      "outputs": [],
      "source": [
        "LR_tfid_svd_tm = TrainedModel(LR_tfid_svd, f\"LR + TFID + SVD ({N_COMPONENTS})\", x_tfid_test_svd, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZe6r8DhOU6H"
      },
      "outputs": [],
      "source": [
        "LR_count_svd_tm = TrainedModel(LR_count_svd, f\"LR + Count + SVD ({N_COMPONENTS})\", x_count_test_svd, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x9GeMKoOU6H"
      },
      "outputs": [],
      "source": [
        "KNN_tfid_svd_tm = TrainedModel(KNN_tfid_svd, f\"KNN ({KNN_tfid_svd.n_neighbors}) + TFID + SVD ({N_COMPONENTS})\", x_tfid_test_svd, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbXgoz0yOU6I"
      },
      "outputs": [],
      "source": [
        "KNN_count_svd_tm = TrainedModel(KNN_count_svd, f\"KNN ({KNN_count_svd.n_neighbors}) + Count + SVD ({N_COMPONENTS})\", x_count_test_svd, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K_wILvPOU6I"
      },
      "outputs": [],
      "source": [
        "trained_models = [\n",
        "    LR_tfid_tm, LR_tfid_tm, LR_tfid_svd_tm, LR_count_svd_tm, KNN_tfid_svd_tm, KNN_count_svd_tm\n",
        "]\n",
        "labels = [0, 1]\n",
        "target_names = [\"Fake\", \"True\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S18_sxkROU6J"
      },
      "source": [
        "### Reportes de clasificación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtfNxOjOOU6J"
      },
      "outputs": [],
      "source": [
        "def plot_clasification_reports(trained_models, labels, target_names, cols=2):\n",
        "    # Se generan los reportes\n",
        "    reports = [\n",
        "        pd.DataFrame(\n",
        "            classification_report(\n",
        "                m.y_test,\n",
        "                m.predict,\n",
        "                labels=labels,\n",
        "                target_names=target_names,\n",
        "                output_dict=True,\n",
        "            )\n",
        "        )\n",
        "        for m in trained_models\n",
        "    ]\n",
        "\n",
        "    # Se obtiene la norma de colores de todos los modelos\n",
        "    # https://stackoverflow.com/a/70517313/15217078\n",
        "    values = np.hstack([d.iloc[:-1, :].values.ravel() for d in reports])\n",
        "    norm = mcolors.Normalize(values.min(), values.max())\n",
        "\n",
        "    # Se generan las gráficas de los reportes\n",
        "    # # https://stackoverflow.com/a/58948133/15217078\n",
        "    rows = int(np.ceil(len(trained_models) / cols))\n",
        "    fig, axes = plt.subplots(rows, cols)\n",
        "    fig.set_size_inches(8 * cols, 5 * rows)\n",
        "    for i, m in enumerate(trained_models):\n",
        "        ax = axes[i // cols][i % cols]\n",
        "        r = reports[i]\n",
        "        sns.heatmap(r.iloc[:-1, :].T, annot=True, norm=norm, ax=ax)\n",
        "        ax.set_title(m.name)\n",
        "    return fig, axes\n",
        "\n",
        "\n",
        "report_fig, report_axes = plot_clasification_reports(\n",
        "    trained_models, labels, target_names\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prueba para clasificación usando mezcla de vectorizaciones (Word2Vec y CountVectorizer)"
      ],
      "metadata": {
        "id": "kwM1QT2WI8iB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "JELK1gWhO3WJ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Datos de ejemplo\n",
        "lst = x_train.tolist()\n",
        "text_data = lst[:50]\n",
        "labels = y_train.tolist()\n",
        "labels = labels[:50]"
      ],
      "metadata": {
        "id": "ZpMVZpCeI6zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorización con CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "count_vectors = count_vectorizer.fit_transform(text_data).toarray()"
      ],
      "metadata": {
        "id": "CgHih1w_R5gw"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorización con WordEmbeddings (Word2Vec)\n",
        "word2vec_model = Word2Vec([text.split() for text in text_data], min_count=1)\n",
        "embedding_vectors = np.array([np.mean([word2vec_model.wv[word] for word in text.split()], axis=0) for text in text_data])"
      ],
      "metadata": {
        "id": "IB7Dd4QBR8Kw"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combinar las vectorizaciones\n",
        "combined_vectors = np.concatenate((count_vectors, embedding_vectors), axis=1)\n",
        "\n",
        "# Entrenar el modelo de regresión logística\n",
        "logistic_regression = LogisticRegression()\n",
        "logistic_regression.fit(combined_vectors, labels)"
      ],
      "metadata": {
        "id": "eIbdebkfR-lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de predicción\n",
        "new_text = 'This is a fucking fake new.'\n",
        "# Preporcesamiento de texto de prueba\n",
        "new_count_vector = count_vectorizer.transform([new_text]).toarray()\n",
        "new_embedding_vector = np.mean([word2vec_model.wv[word] for word in new_text.split()], axis=0)\n",
        "new_combined_vector = np.concatenate((new_count_vector, np.array([new_embedding_vector])), axis=1)\n",
        "prediction = logistic_regression.predict(new_combined_vector)\n",
        "\n",
        "print(f'La predicción para el nuevo texto \"{new_text}\" es: {prediction}')"
      ],
      "metadata": {
        "id": "8gcRzkkvRsz-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}